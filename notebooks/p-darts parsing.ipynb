{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "international-chile",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "\n",
    "import scipy.special\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electrical-florist",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Convert the notebook to /tests directory\n",
    "n_ops = 7\n",
    "n_nodes = 3\n",
    "\n",
    "alpha_normal = []\n",
    "alpha_reduce = []\n",
    "\n",
    "for i in range(n_nodes):\n",
    "    # create alpha parameters over parallel operations\n",
    "    alpha_normal.append(nn.Parameter(\n",
    "        1e-3 * torch.randn(i + 2, n_ops)))\n",
    "    alpha_reduce.append(nn.Parameter(\n",
    "        1e-3 * torch.randn(i + 2, n_ops)))\n",
    "    \n",
    "    \n",
    "[i.shape for i in alpha_normal]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conscious-woman",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set alphas to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exact-broad",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = []\n",
    "alpha_reduce = []\n",
    "n_ops = 7\n",
    "\n",
    "class A(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(A, self).__init__()\n",
    "        self.alphas = []\n",
    "        for i in range(n_nodes):\n",
    "            # create alpha parameters over parallel operations\n",
    "            self.alphas.append(nn.Parameter(\n",
    "                1e-3 * torch.randn(i + 2, n_ops)))\n",
    "            alpha_reduce.append(nn.Parameter(\n",
    "                1e-3 * torch.randn(i + 2, n_ops)))\n",
    "\n",
    "# alphas = [torch.tensor(np.random.normal(0, 0.4, size=(2,7))),\n",
    "#          torch.tensor(np.random.normal(0, 0.4, size=(3,7))), \n",
    "#          torch.tensor(np.random.normal(0, 0.4, size=(4,7)))]\n",
    "\n",
    "\n",
    "# drop = np.array(weights[i, :]).argsort()[:ops_drop]\n",
    "num_to_drop = [2, 2, 0]\n",
    "A = A()\n",
    "# add stages loop\n",
    "# TODO: Currently the 0 values are sometimes higher than negative values that might show up\n",
    "for stage in range(3):\n",
    "    n_ops_reduce = 5 - num_to_drop[stage]\n",
    "\n",
    "    for alpha in A.alphas:\n",
    "        topk, indices = torch.topk(alpha[:, :], n_ops_reduce)\n",
    "\n",
    "        mask = torch.zeros(len(alpha), n_ops)\n",
    "        a_min = torch.min(alpha)\n",
    "        epsilon = 0.0001\n",
    "        print(alpha)\n",
    "        print(indices)\n",
    "#         print(mask)\n",
    "#         print(indices)\n",
    "#         print(mask.scatter_(1, indices, True) * alpha)\n",
    "        alpha.data = mask.scatter_(1, indices, True) * alpha\n",
    "        alpha.data[alpha.data == 0] = a_min - epsilon\n",
    "        print(alpha)\n",
    "        break\n",
    "    break\n",
    "        \n",
    "#     print(alphas, \"\\n\")\n",
    "        \n",
    "# A.alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infectious-framework",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for skip-connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "utility-conclusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRIMITIVES = [\n",
    "    \"max_pool_3x3\",\n",
    "    \"avg_pool_3x3\",\n",
    "    \"skip_connect\",  # identity\n",
    "    \"conv_1x5_5x1\",\n",
    "    \"conv_3x3\",\n",
    "    \"sep_conv_3x3\",\n",
    "    \"dil_conv_3x3\"\n",
    "]\n",
    "\n",
    "alpha = [torch.tensor(np.random.normal(0, 0.4, size=(2,7))),\n",
    "         torch.tensor(np.random.normal(0, 0.4, size=(3,7))), \n",
    "         torch.tensor(np.random.normal(0, 0.4, size=(4,7)))]\n",
    "\n",
    "alpha_pairwise = [torch.tensor([1]), \n",
    "                  torch.tensor([0.3346, 0., 0.]), \n",
    "                  torch.tensor([0.0, 0.0, 0.0, 0.1686, 0.0, 0.0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acoustic-celtic",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_tensor_alphas(alpha_concat, nodes=3):\n",
    "    alphas = []\n",
    "    for a_i in get_edge_indices(nodes):\n",
    "        alphas.append(\n",
    "            torch.Tensor(alpha_concat[a_i[0]:a_i[1]]))\n",
    "        \n",
    "    # print(alpha_concat, alphas)\n",
    "    return alphas\n",
    "\n",
    "def get_edge_indices(nodes=3):\n",
    "    # Amount of nodes for each edge\n",
    "    j = [i for i in range(2, nodes+2)] \n",
    "    \n",
    "    prev = 0\n",
    "    indices = []\n",
    "    for i in j:\n",
    "        if prev != 0:\n",
    "            indices.append((sum(j[:j.index(prev)+1]), \n",
    "                            sum(j[:j.index(i)+1])))\n",
    "        else:\n",
    "            indices.append((0, sum(j[:j.index(i)+1])))\n",
    "        prev = i\n",
    "    return indices\n",
    "\n",
    "def parse(alpha, k, primitives=PRIMITIVES):\n",
    "    gene = []\n",
    "    for edges in alpha:\n",
    "        # edges: Tensor(n_edges, n_ops)\n",
    "        edge_max, primitive_indices = torch.topk(\n",
    "            edges[:, :], 1\n",
    "        )  # ignore 'none' ##removed none\n",
    "        topk_edge_values, topk_edge_indices = torch.topk(edge_max.view(-1), k)\n",
    "        node_gene = []\n",
    "        for edge_idx in topk_edge_indices:\n",
    "            prim_idx = primitive_indices[edge_idx]\n",
    "            prim = primitives[prim_idx]\n",
    "            node_gene.append((prim, edge_idx.item()))\n",
    "\n",
    "        gene.append(node_gene)\n",
    "    return gene\n",
    "\n",
    "\n",
    "parse(alpha, 2, switches_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grave-examination",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "weights_normal = [F.softmax(a).data.cpu().numpy() for a in alpha]\n",
    "weights_normal = np.concatenate(weights_normal, axis=0)\n",
    "switches = switches_normal\n",
    "\n",
    "idxs = np.where(switches[i])[0].tolist()\n",
    "drop = np.array(weights_normal[i, :]).argsort()[:2]\n",
    "\n",
    "\n",
    "idxs, drop, weights_normal[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brutal-strip",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_edge_indices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enormous-looking",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha\n",
    "\n",
    "[a.shape for a in alpha]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "small-biography",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = 3\n",
    "\n",
    "def get_edge_indices(nodes=3):\n",
    "    # Amount of nodes for each edge\n",
    "    j = [i for i in range(2, nodes+2)] \n",
    "    \n",
    "    prev = 0\n",
    "    indices = []\n",
    "    for i in j:\n",
    "        if prev != 0:\n",
    "            indices.append((sum(j[:j.index(prev)+1]), \n",
    "                            sum(j[:j.index(i)+1])))\n",
    "        else:\n",
    "            indices.append((0, sum(j[:j.index(i)+1])))\n",
    "        prev = i\n",
    "    return indices\n",
    "\n",
    "def find_indice(row_idx, nodes=3):\n",
    "    edges = get_edge_indices(nodes)\n",
    "\n",
    "    for i, (lower, upper) in enumerate(edges):\n",
    "        if row_idx >= lower and row_idx <= upper:\n",
    "            # return index\n",
    "            idx_alpha = 0 if row_idx-lower == 0 else row_idx-lower-1\n",
    "            return i, idx_alpha\n",
    "        \n",
    "# alpha[][]\n",
    "for i in range(0, 10):\n",
    "    print(find_indice(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "powered-enough",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automated-china",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "alpha = [torch.tensor(np.random.normal(0, 0.4, size=(2,7))).cuda(),\n",
    "         torch.tensor(np.random.normal(0, 0.4, size=(3,7))).cuda(), \n",
    "         torch.tensor(np.random.normal(0, 0.4, size=(4,7))).cuda()]\n",
    "alpha[0][0][2] = 1\n",
    "alpha[0][1][2] = 1\n",
    "alpha[1][0][2] = 1\n",
    "alpha[1][1][2] = 1\n",
    "alpha[2][0][2] = 1\n",
    "alpha[2][1][2] = 1\n",
    "\n",
    "print(np.concatenate(\n",
    "    [a.detach().cpu().numpy() for a in alpha],\n",
    "    axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upset-collaboration",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_edge_indices(nodes=3):\n",
    "    # Amount of nodes for each edge\n",
    "    j = [i for i in range(2, nodes+2)] \n",
    "    \n",
    "    prev = 0\n",
    "    indices = []\n",
    "    for i in j:\n",
    "        if prev != 0:\n",
    "            indices.append((sum(j[:j.index(prev)+1]), \n",
    "                            sum(j[:j.index(i)+1])))\n",
    "        else:\n",
    "            indices.append((0, sum(j[:j.index(i)+1])))\n",
    "        prev = i\n",
    "    return indices\n",
    "\n",
    "def find_indice(row_idx, nodes=3):\n",
    "    edges = get_edge_indices(nodes)\n",
    "\n",
    "    for i, (lower, upper) in enumerate(edges):\n",
    "        if row_idx >= lower and row_idx <= upper:\n",
    "            # return index\n",
    "            idx_alpha = 0 if row_idx-lower == 0 else row_idx-lower-1\n",
    "            return i, idx_alpha\n",
    "\n",
    "def limit_skip_connections_alphas(alpha, primitives, k=2, nodes=3, num_of_skip_connections=2):\n",
    "    \"\"\"Mutate the alpha in-place and return the corresponding gene\"\"\"\n",
    "    gene = parse(alpha, k=k)\n",
    "    num_sk_enabled = sum([1 for edge in gene\n",
    "                          for op in edge if op[0] == \"skip_connect\"])\n",
    "    epsilon = 0.00\n",
    "    \n",
    "    if num_sk_enabled < num_of_skip_connections:\n",
    "        return gene\n",
    "    else:\n",
    "        sk_idx = primitives.index(\"skip_connect\")\n",
    "        alphas_concat = np.concatenate(\n",
    "            [a.detach().numpy() for a in alpha],\n",
    "            axis=0)\n",
    "        sk_alphas = alphas_concat[:, sk_idx-1:sk_idx]\n",
    "\n",
    "        for i in range(len(sk_alphas)):\n",
    "            # Pick skip-connection index with lowest alpha value\n",
    "            row_idx = np.argmin(sk_alphas)\n",
    "            # Set to inf so we don't pick this again\n",
    "            sk_alphas[row_idx] = float(\"inf\")\n",
    "            \n",
    "            edge_idx, row_idx = find_indice(row_idx)\n",
    "            print(alpha[edge_idx][row_idx])\n",
    "            print(torch.min(alpha[edge_idx][row_idx]))\n",
    "\n",
    "            alpha[edge_idx][row_idx][sk_idx] = torch.min(alpha[edge_idx][row_idx]) - epsilon\n",
    "            gene = parse(alpha, k=k)\n",
    "            \n",
    "            num_sk_enabled = sum([1 for edge in gene\n",
    "                                  for op in edge if op[0] == \"skip_connect\"])\n",
    "            if num_sk_enabled < num_of_skip_connections:\n",
    "                return gene\n",
    "\n",
    "limit_skip_connections_alphas(alpha, PRIMITIVES)\n",
    "print(np.concatenate(\n",
    "    [a.detach().numpy() for a in alpha],\n",
    "    axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nasty-captain",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def reduce_skip_connections(alphas, num_of_skip_connections=2, nodes=3, \n",
    "                           k=2, primitives=PRIMITIVES):\n",
    "    sk_idx = primitives.index(\"skip_connect\")\n",
    "    alphas = [a.detach().numpy() for a in alphas]\n",
    "    alpha_concat = np.concatenate(alphas, axis=0)\n",
    "    \n",
    "    # Number of skip-connections enabled in switches\n",
    "    gene = parse(alphas, switches, k=k)\n",
    "    num_sk_enabled = sum([1 for edge in gene \n",
    "                          for op in edge if op[0] == \"skip_connect\"])\n",
    "    \n",
    "    sk_a = alpha_concat[:, sk_idx-1:sk_idx]\n",
    "    if num_sk_enabled < num_of_skip_connections:\n",
    "        alphas = convert_tensor_alphas(alpha_concat)\n",
    "        gene = parse(alphas, switches, k=k)\n",
    "        return gene\n",
    "    else:\n",
    "        it = 0\n",
    "        while num_sk_enabled > num_of_skip_connections:\n",
    "            print(\"########## iteration\", it)\n",
    "            it += 1\n",
    "            # Pick skip-connection index with lowest alpha \n",
    "            # value\n",
    "            row_idx = np.argmin(sk_a)\n",
    "            sk_a[row_idx] = float(\"inf\")\n",
    "\n",
    "        #     # set alphas to -inf to make sure, prevent it from\n",
    "        #     # being picked. \n",
    "            alpha_concat[row_idx][sk_idx] = float(\"-inf\")\n",
    "            alphas = convert_tensor_alphas(alpha_concat)\n",
    "\n",
    "            gene = parse(alphas, switches, k=k)\n",
    "            num_sk_enabled = sum([1 for edge in gene \n",
    "                                  for op in edge if op[0] == \"skip_connect\"])\n",
    "\n",
    "            if num_sk_enabled <= num_of_skip_connections:\n",
    "                # return the new switches\n",
    "                return gene\n",
    "\n",
    "limit_skip_connections(alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "presidential-elizabeth",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "alphas = [torch.tensor(np.random.normal(0, 0.4, size=(2,3))).cuda(),\n",
    "         torch.tensor(np.random.normal(0, 0.4, size=(3,3))).cuda(), \n",
    "         torch.tensor(np.random.normal(0, 0.4, size=(4,3))).cuda()]\n",
    "\n",
    "alphas = [a.detach().cpu().numpy() for a in alphas]\n",
    "alpha_concat = np.concatenate(alphas, axis=0)\n",
    "alpha_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modular-metadata",
   "metadata": {},
   "outputs": [],
   "source": [
    "sk_a = alpha_concat[:, 1:2]\n",
    "\n",
    "np.argmin(sk_a)\n",
    "\n",
    "it = 0\n",
    "while num_sk_enabled > num_of_sk:\n",
    "    print(\"########## iteration\", it)\n",
    "    it += 1\n",
    "    # Pick skip-connection index with lowest alpha \n",
    "    # value\n",
    "    row_idx = np.argmin(sk_a)\n",
    "    sk_a[row_idx] = float(\"inf\")\n",
    "\n",
    "#     # set alphas to -inf to make sure, prevent it from\n",
    "#     # being picked. \n",
    "    alpha_concat[row_idx][sk_idx] = float(\"-inf\")\n",
    "    alphas = convert_tensor_alphas(alpha_concat)\n",
    "\n",
    "    gene = parse(alphas, switches, k=k)\n",
    "    num_sk_enabled = sum([1 for edge in gene \n",
    "                          for op in edge if op[0] == \"skip_connect\"])\n",
    "\n",
    "    if num_sk_enabled <= num_of_sk:\n",
    "        # return the new switches\n",
    "        return gene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clean-priest",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('meta': conda)",
   "language": "python",
   "name": "python3710jvsc74a57bd0b1d57bbef129b95556cf4acac245eaf539d69532a51fcbf5e76efb5e83c89ceb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
