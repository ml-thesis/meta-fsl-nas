{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "paperback-macro",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "\n",
    "import scipy.special\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "sticky-transparency",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([2, 7]), torch.Size([3, 7]), torch.Size([4, 7])]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Convert the notebook to /tests directory\n",
    "n_ops = 7\n",
    "n_nodes = 3\n",
    "\n",
    "alpha_normal = []\n",
    "alpha_reduce = []\n",
    "\n",
    "for i in range(n_nodes):\n",
    "    # create alpha parameters over parallel operations\n",
    "    alpha_normal.append(nn.Parameter(\n",
    "        1e-3 * torch.randn(i + 2, n_ops)))\n",
    "    alpha_reduce.append(nn.Parameter(\n",
    "        1e-3 * torch.randn(i + 2, n_ops)))\n",
    "    \n",
    "    \n",
    "[i.shape for i in alpha_normal]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cultural-pollution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set alphas to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "accompanied-geology",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0013,  0.0006,  0.0001,  0.0007, -0.0015,  0.0002,  0.0010],\n",
      "        [ 0.0008,  0.0005, -0.0027, -0.0006,  0.0006, -0.0011, -0.0011]],\n",
      "       requires_grad=True)\n",
      "tensor([[6, 3, 1],\n",
      "        [0, 4, 1]])\n",
      "Parameter containing:\n",
      "tensor([[-0.0028,  0.0006, -0.0028,  0.0007, -0.0028, -0.0028,  0.0010],\n",
      "        [ 0.0008,  0.0005, -0.0028, -0.0028,  0.0006, -0.0028, -0.0028]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "alphas = []\n",
    "alpha_reduce = []\n",
    "n_ops = 7\n",
    "\n",
    "class A(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(A, self).__init__()\n",
    "        self.alphas = []\n",
    "        for i in range(n_nodes):\n",
    "            # create alpha parameters over parallel operations\n",
    "            self.alphas.append(nn.Parameter(\n",
    "                1e-3 * torch.randn(i + 2, n_ops)))\n",
    "            alpha_reduce.append(nn.Parameter(\n",
    "                1e-3 * torch.randn(i + 2, n_ops)))\n",
    "\n",
    "# alphas = [torch.tensor(np.random.normal(0, 0.4, size=(2,7))),\n",
    "#          torch.tensor(np.random.normal(0, 0.4, size=(3,7))), \n",
    "#          torch.tensor(np.random.normal(0, 0.4, size=(4,7)))]\n",
    "\n",
    "\n",
    "# drop = np.array(weights[i, :]).argsort()[:ops_drop]\n",
    "num_to_drop = [2, 2, 0]\n",
    "A = A()\n",
    "# add stages loop\n",
    "# TODO: Currently the 0 values are sometimes higher than negative values that might show up\n",
    "for stage in range(3):\n",
    "    n_ops_reduce = 5 - num_to_drop[stage]\n",
    "\n",
    "    for alpha in A.alphas:\n",
    "        topk, indices = torch.topk(alpha[:, :], n_ops_reduce)\n",
    "\n",
    "        mask = torch.zeros(len(alpha), n_ops)\n",
    "        a_min = torch.min(alpha)\n",
    "        epsilon = 0.0001\n",
    "        print(alpha)\n",
    "        print(indices)\n",
    "#         print(mask)\n",
    "#         print(indices)\n",
    "#         print(mask.scatter_(1, indices, True) * alpha)\n",
    "        alpha.data = mask.scatter_(1, indices, True) * alpha\n",
    "        alpha.data[alpha.data == 0] = a_min - epsilon\n",
    "        print(alpha)\n",
    "        break\n",
    "    break\n",
    "        \n",
    "#     print(alphas, \"\\n\")\n",
    "        \n",
    "# A.alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organic-module",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for skip-connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fitting-contamination",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRIMITIVES = [\n",
    "    \"max_pool_3x3\",\n",
    "    \"avg_pool_3x3\",\n",
    "    \"skip_connect\",  # identity\n",
    "    \"conv_1x5_5x1\",\n",
    "    \"conv_3x3\",\n",
    "    \"sep_conv_3x3\",\n",
    "    \"dil_conv_3x3\"\n",
    "]\n",
    "\n",
    "alpha = [torch.tensor(np.random.normal(0, 0.4, size=(2,7))),\n",
    "         torch.tensor(np.random.normal(0, 0.4, size=(3,7))), \n",
    "         torch.tensor(np.random.normal(0, 0.4, size=(4,7)))]\n",
    "\n",
    "alpha_pairwise = [torch.tensor([1]), \n",
    "                  torch.tensor([0.3346, 0., 0.]), \n",
    "                  torch.tensor([0.0, 0.0, 0.0, 0.1686, 0.0, 0.0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "common-participation",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'switches_normal' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-c128d704e126>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswitches_normal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'switches_normal' is not defined"
     ]
    }
   ],
   "source": [
    "def convert_tensor_alphas(alpha_concat, nodes=3):\n",
    "    alphas = []\n",
    "    for a_i in get_edge_indices(nodes):\n",
    "        alphas.append(\n",
    "            torch.Tensor(alpha_concat[a_i[0]:a_i[1]]))\n",
    "        \n",
    "    # print(alpha_concat, alphas)\n",
    "    return alphas\n",
    "\n",
    "def get_edge_indices(nodes=3):\n",
    "    # Amount of nodes for each edge\n",
    "    j = [i for i in range(2, nodes+2)] \n",
    "    \n",
    "    prev = 0\n",
    "    indices = []\n",
    "    for i in j:\n",
    "        if prev != 0:\n",
    "            indices.append((sum(j[:j.index(prev)+1]), \n",
    "                            sum(j[:j.index(i)+1])))\n",
    "        else:\n",
    "            indices.append((0, sum(j[:j.index(i)+1])))\n",
    "        prev = i\n",
    "    return indices\n",
    "\n",
    "def parse(alpha, k, primitives=PRIMITIVES):\n",
    "    gene = []\n",
    "    for edges in alpha:\n",
    "        # edges: Tensor(n_edges, n_ops)\n",
    "        edge_max, primitive_indices = torch.topk(\n",
    "            edges[:, :], 1\n",
    "        )  # ignore 'none' ##removed none\n",
    "        topk_edge_values, topk_edge_indices = torch.topk(edge_max.view(-1), k)\n",
    "        node_gene = []\n",
    "        for edge_idx in topk_edge_indices:\n",
    "            prim_idx = primitive_indices[edge_idx]\n",
    "            prim = primitives[prim_idx]\n",
    "            node_gene.append((prim, edge_idx.item()))\n",
    "\n",
    "        gene.append(node_gene)\n",
    "    return gene\n",
    "\n",
    "\n",
    "parse(alpha, 2, switches_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unsigned-street",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "weights_normal = [F.softmax(a).data.cpu().numpy() for a in alpha]\n",
    "weights_normal = np.concatenate(weights_normal, axis=0)\n",
    "switches = switches_normal\n",
    "\n",
    "idxs = np.where(switches[i])[0].tolist()\n",
    "drop = np.array(weights_normal[i, :]).argsort()[:2]\n",
    "\n",
    "\n",
    "idxs, drop, weights_normal[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "separated-asset",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 2), (2, 5), (5, 9)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_edge_indices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "floating-intranet",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([2, 7]), torch.Size([3, 7]), torch.Size([4, 7])]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha\n",
    "\n",
    "[a.shape for a in alpha]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "lucky-hierarchy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 1)\n",
      "(1, 0)\n",
      "(1, 1)\n",
      "(1, 2)\n",
      "(2, 0)\n",
      "(2, 1)\n",
      "(2, 2)\n",
      "(2, 3)\n"
     ]
    }
   ],
   "source": [
    "nodes = 3\n",
    "\n",
    "def get_edge_indices(nodes=3):\n",
    "    # Amount of nodes for each edge\n",
    "    j = [i for i in range(2, nodes+2)] \n",
    "    \n",
    "    prev = 0\n",
    "    indices = []\n",
    "    for i in j:\n",
    "        if prev != 0:\n",
    "            indices.append((sum(j[:j.index(prev)+1]), \n",
    "                            sum(j[:j.index(i)+1])))\n",
    "        else:\n",
    "            indices.append((0, sum(j[:j.index(i)+1])))\n",
    "        prev = i\n",
    "    return indices\n",
    "\n",
    "def find_indice(row_idx, nodes=3):\n",
    "    edges = get_edge_indices(nodes)\n",
    "\n",
    "    for i, (lower, upper) in enumerate(edges):\n",
    "        if row_idx >= lower and row_idx <= upper:\n",
    "            # return index\n",
    "            idx_alpha = 0 if row_idx-lower == 0 else row_idx-lower-1\n",
    "            return i, idx_alpha\n",
    "        \n",
    "# alpha[][]\n",
    "for i in range(0, 10):\n",
    "    print(find_indice(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "presidential-comparison",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "black-somewhere",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.54005336  0.44601386  1.          0.16935927  0.61018918 -0.09079537\n",
      "   0.05649533]\n",
      " [ 0.17622196 -0.83845706  1.          0.11027397 -0.00795754  0.30650679\n",
      "  -0.34431568]\n",
      " [-0.86343314  0.22569936  1.          0.4516483  -0.26683228 -0.34520844\n",
      "   0.58705117]\n",
      " [ 0.07984683  0.58052784  1.         -0.42695922  0.59414872  0.73740815\n",
      "  -0.17766901]\n",
      " [ 0.34435377 -0.26419923 -0.41083582 -0.1372797   0.28563996 -0.20738398\n",
      "   0.63491066]\n",
      " [-0.24708575  0.04247829  1.         -0.54031261  0.25097981  0.22914088\n",
      "   0.19957397]\n",
      " [ 0.17441212  0.0447187   1.          0.12966215  0.51792497  0.32531224\n",
      "  -0.54569785]\n",
      " [-0.3830318  -0.46992206  0.09301063  0.83863576  0.30681551 -0.00942102\n",
      "  -0.26645276]\n",
      " [-0.52736485 -0.33109814  0.13805327 -0.8320009   0.27071955  0.12108847\n",
      "   0.33667816]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "alpha = [torch.tensor(np.random.normal(0, 0.4, size=(2,7))).cuda(),\n",
    "         torch.tensor(np.random.normal(0, 0.4, size=(3,7))).cuda(), \n",
    "         torch.tensor(np.random.normal(0, 0.4, size=(4,7))).cuda()]\n",
    "alpha[0][0][2] = 1\n",
    "alpha[0][1][2] = 1\n",
    "alpha[1][0][2] = 1\n",
    "alpha[1][1][2] = 1\n",
    "alpha[2][0][2] = 1\n",
    "alpha[2][1][2] = 1\n",
    "\n",
    "print(np.concatenate(\n",
    "    [a.detach().cpu().numpy() for a in alpha],\n",
    "    axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "suitable-motorcycle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.19708896  0.1437415   0.08133781  0.13216552 -0.66782436  0.01566144\n",
      "   0.28501196]\n",
      " [-0.06607365 -0.39054451  0.16713766  0.43814778  0.11344568 -0.43010951\n",
      "  -0.49521541]\n",
      " [-0.10706372  0.15323234 -0.4360023   0.77714112  0.10937363 -0.68392082\n",
      "  -0.0880985 ]\n",
      " [-0.78684685 -0.36010764  0.29144824  0.0575596   0.01070957  0.19358707\n",
      "  -0.10091347]\n",
      " [-0.09255316  0.56303111  0.62339725 -0.72657522 -0.02643146  0.32926354\n",
      "   0.12097027]\n",
      " [ 0.29802936  0.49010262 -0.40068117 -0.01854009  0.22592499 -0.49414736\n",
      "   0.15203245]\n",
      " [-0.38684428  0.5841493   0.51792072 -0.27329896 -0.19433815  0.55878352\n",
      "  -0.06745332]\n",
      " [ 0.21673819 -0.28628213  0.         -0.87630712  0.15935514  0.16476185\n",
      "   0.10258931]\n",
      " [ 0.30347341 -0.39530993  0.14879027  0.30460576 -0.05406766 -0.41679011\n",
      "  -0.52559684]]\n"
     ]
    }
   ],
   "source": [
    "def get_edge_indices(nodes=3):\n",
    "    # Amount of nodes for each edge\n",
    "    j = [i for i in range(2, nodes+2)] \n",
    "    \n",
    "    prev = 0\n",
    "    indices = []\n",
    "    for i in j:\n",
    "        if prev != 0:\n",
    "            indices.append((sum(j[:j.index(prev)+1]), \n",
    "                            sum(j[:j.index(i)+1])))\n",
    "        else:\n",
    "            indices.append((0, sum(j[:j.index(i)+1])))\n",
    "        prev = i\n",
    "    return indices\n",
    "\n",
    "def find_indice(row_idx, nodes=3):\n",
    "    edges = get_edge_indices(nodes)\n",
    "\n",
    "    for i, (lower, upper) in enumerate(edges):\n",
    "        if row_idx >= lower and row_idx <= upper:\n",
    "            # return index\n",
    "            idx_alpha = 0 if row_idx-lower == 0 else row_idx-lower-1\n",
    "            return i, idx_alpha\n",
    "\n",
    "def limit_skip_connections_alphas(alpha, primitives, k=2, nodes=3, num_of_skip_connections=2):\n",
    "    \"\"\"Mutate the alpha in-place and return the corresponding gene\"\"\"\n",
    "    gene = parse(alpha, k=k)\n",
    "    num_sk_enabled = sum([1 for edge in gene\n",
    "                          for op in edge if op[0] == \"skip_connect\"])\n",
    "    epsilon = 0.00\n",
    "    \n",
    "    if num_sk_enabled < num_of_skip_connections:\n",
    "        return gene\n",
    "    else:\n",
    "        sk_idx = primitives.index(\"skip_connect\")\n",
    "        alphas_concat = np.concatenate(\n",
    "            [a.detach().numpy() for a in alpha],\n",
    "            axis=0)\n",
    "        sk_alphas = alphas_concat[:, sk_idx-1:sk_idx]\n",
    "\n",
    "        for i in range(len(sk_alphas)):\n",
    "            # Pick skip-connection index with lowest alpha value\n",
    "            row_idx = np.argmin(sk_alphas)\n",
    "            # Set to inf so we don't pick this again\n",
    "            sk_alphas[row_idx] = float(\"inf\")\n",
    "            \n",
    "            edge_idx, row_idx = find_indice(row_idx)\n",
    "            print(alpha[edge_idx][row_idx])\n",
    "            print(torch.min(alpha[edge_idx][row_idx]))\n",
    "\n",
    "            alpha[edge_idx][row_idx][sk_idx] = torch.min(alpha[edge_idx][row_idx]) - epsilon\n",
    "            gene = parse(alpha, k=k)\n",
    "            \n",
    "            num_sk_enabled = sum([1 for edge in gene\n",
    "                                  for op in edge if op[0] == \"skip_connect\"])\n",
    "            if num_sk_enabled < num_of_skip_connections:\n",
    "                return gene\n",
    "\n",
    "limit_skip_connections_alphas(alpha, PRIMITIVES)\n",
    "print(np.concatenate(\n",
    "    [a.detach().numpy() for a in alpha],\n",
    "    axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "inside-biography",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PRIMITIVES' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-df99de133f1e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m def limit_skip_connections(alphas, num_of_sk=2, nodes=3, \n\u001b[0;32m----> 2\u001b[0;31m                            k=2, primitives=PRIMITIVES):\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0msk_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprimitives\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"skip_connect\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0malphas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0malphas\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0malpha_concat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malphas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'PRIMITIVES' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "def reduce_skip_connections(alphas, num_of_skip_connections=2, nodes=3, \n",
    "                           k=2, primitives=PRIMITIVES):\n",
    "    sk_idx = primitives.index(\"skip_connect\")\n",
    "    alphas = [a.detach().numpy() for a in alphas]\n",
    "    alpha_concat = np.concatenate(alphas, axis=0)\n",
    "    \n",
    "    # Number of skip-connections enabled in switches\n",
    "    gene = parse(alphas, switches, k=k)\n",
    "    num_sk_enabled = sum([1 for edge in gene \n",
    "                          for op in edge if op[0] == \"skip_connect\"])\n",
    "    \n",
    "    sk_a = alpha_concat[:, sk_idx-1:sk_idx]\n",
    "    if num_sk_enabled < num_of_skip_connections:\n",
    "        alphas = convert_tensor_alphas(alpha_concat)\n",
    "        gene = parse(alphas, switches, k=k)\n",
    "        return gene\n",
    "    else:\n",
    "        it = 0\n",
    "        while num_sk_enabled > num_of_skip_connections:\n",
    "            print(\"########## iteration\", it)\n",
    "            it += 1\n",
    "            # Pick skip-connection index with lowest alpha \n",
    "            # value\n",
    "            row_idx = np.argmin(sk_a)\n",
    "            sk_a[row_idx] = float(\"inf\")\n",
    "\n",
    "        #     # set alphas to -inf to make sure, prevent it from\n",
    "        #     # being picked. \n",
    "            alpha_concat[row_idx][sk_idx] = float(\"-inf\")\n",
    "            alphas = convert_tensor_alphas(alpha_concat)\n",
    "\n",
    "            gene = parse(alphas, switches, k=k)\n",
    "            num_sk_enabled = sum([1 for edge in gene \n",
    "                                  for op in edge if op[0] == \"skip_connect\"])\n",
    "\n",
    "            if num_sk_enabled <= num_of_skip_connections:\n",
    "                # return the new switches\n",
    "                return gene\n",
    "\n",
    "limit_skip_connections(alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dramatic-enforcement",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.47157174,  0.0242357 ,  0.40998554],\n",
       "       [-0.2201242 , -0.36017159,  0.26092069],\n",
       "       [-0.428018  , -0.56124322, -0.90665213],\n",
       "       [-0.30024917,  1.09211891,  0.39520035],\n",
       "       [ 0.57640854, -0.12008687,  0.08212192],\n",
       "       [ 0.17091093, -0.70449669, -0.36777858],\n",
       "       [-0.44876285,  0.31320018,  0.43844799],\n",
       "       [-0.52653206, -0.34502978, -0.13791534],\n",
       "       [-0.06752017,  0.63286921, -0.00976202]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "alphas = [torch.tensor(np.random.normal(0, 0.4, size=(2,3))).cuda(),\n",
    "         torch.tensor(np.random.normal(0, 0.4, size=(3,3))).cuda(), \n",
    "         torch.tensor(np.random.normal(0, 0.4, size=(4,3))).cuda()]\n",
    "\n",
    "alphas = [a.detach().cpu().numpy() for a in alphas]\n",
    "alpha_concat = np.concatenate(alphas, axis=0)\n",
    "alpha_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "thirty-crime",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.43544406],\n",
       "       [-0.26283069],\n",
       "       [-0.26637197],\n",
       "       [-0.25242813],\n",
       "       [-0.02152324],\n",
       "       [-0.89322286],\n",
       "       [-0.29785362],\n",
       "       [ 0.38971684],\n",
       "       [ 0.14580411]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sk_a = alpha_concat[:, 1:2]\n",
    "\n",
    "np.argmin(sk_a)\n",
    "\n",
    "it = 0\n",
    "while num_sk_enabled > num_of_sk:\n",
    "    print(\"########## iteration\", it)\n",
    "    it += 1\n",
    "    # Pick skip-connection index with lowest alpha \n",
    "    # value\n",
    "    row_idx = np.argmin(sk_a)\n",
    "    sk_a[row_idx] = float(\"inf\")\n",
    "\n",
    "#     # set alphas to -inf to make sure, prevent it from\n",
    "#     # being picked. \n",
    "    alpha_concat[row_idx][sk_idx] = float(\"-inf\")\n",
    "    alphas = convert_tensor_alphas(alpha_concat)\n",
    "\n",
    "    gene = parse(alphas, switches, k=k)\n",
    "    num_sk_enabled = sum([1 for edge in gene \n",
    "                          for op in edge if op[0] == \"skip_connect\"])\n",
    "\n",
    "    if num_sk_enabled <= num_of_sk:\n",
    "        # return the new switches\n",
    "        return gene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "collect-merchandise",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C_in = 28\n",
    "C_out = 28\n",
    "kernel_size = 3\n",
    "stride = 1\n",
    "padding = 2\n",
    "dilation = 2\n",
    "C_mid_mult = 1\n",
    "affine = False\n",
    "\n",
    "C_mid = 32\n",
    "cmid = int(C_out * C_mid_mult)\n",
    "cmid = C_mid if C_mid else cmid\n",
    "\n",
    "cmid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "split-world",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): ReLU()\n",
       "  (1): Conv2d(28, 28, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=28, bias=False)\n",
       "  (2): Conv2d(28, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "  (4): ReLU()\n",
       "  (5): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "  (6): Conv2d(32, 28, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (7): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       ")"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "op = nn.Sequential(\n",
    "    nn.ReLU(inplace=False),\n",
    "    nn.Conv2d(C_in, C_in, kernel_size, stride,\n",
    "              padding, dilation, groups=C_in, bias=0),\n",
    "    nn.Conv2d(C_in, cmid, kernel_size=1,\n",
    "              padding=0, bias=0),\n",
    "    nn.BatchNorm2d(cmid, affine=affine),\n",
    "    nn.ReLU(inplace=False),\n",
    "    nn.Conv2d(cmid, cmid, kernel_size,\n",
    "              stride=1, padding=(kernel_size-1)//2,\n",
    "              dilation=1, groups=cmid, bias=0),\n",
    "    nn.Conv2d(cmid, C_out, kernel_size=1,\n",
    "              padding=0, bias=0),\n",
    "    nn.BatchNorm2d(C_out, affine=affine))\n",
    "\n",
    "\n",
    "op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "romantic-karaoke",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]]], grad_fn=<NativeBatchNormBackward>)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "op(torch.zeros((20, 28, 28, 28)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "angry-sampling",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('meta': conda)",
   "language": "python",
   "name": "python3710jvsc74a57bd0b1d57bbef129b95556cf4acac245eaf539d69532a51fcbf5e76efb5e83c89ceb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
